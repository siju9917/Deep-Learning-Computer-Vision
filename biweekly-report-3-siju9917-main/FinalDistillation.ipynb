{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b69cb2a0",
   "metadata": {},
   "source": [
    "# Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "770223c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General Imports\n",
    "import numpy as np\n",
    "\n",
    "#All Imports for Tensorflow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f7bdef",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "### Terminology that we use throughout this section:\n",
    "The \"Teacher\" network is a large and complex neural network that takes a long time to train\n",
    "\n",
    "The \"Student\" network is a small neural network that learns information from the output of the \"Teacher\" network.  This is the network we are attempting to optimize throughout the distillation process because it has far less parameters leading to a reasonable execution time for production products (such as real time object recognition camera systems)\n",
    "\n",
    "The \"Control\" network is a small neural network that has the exact same architecture as the student network.  However we train this network independently so it isn't learning anything from the teacher model and we can use it as a baseline to see if this knowledge distillation process is really making a difference!\n",
    "\n",
    "### The general algorithm that we follow for Knowledge Distillation:\n",
    "1. Define the Teacher and Student Neural Network models (the teacher model should be a lot larger and have more parameters to train\n",
    "2. Train the teacher model using ordinary methods (this will be time consuming most likely depending on how large the teacher model is\n",
    "3. Train the student network model using the \"blurred\" probability classification outputs of the teacher model.  We achieve this blurring by dividing the logits by some temperature value.  Note that THE HIGHER THE TEMPERATURE VALUE THE SOFTER THE PROBABILITY DISTRIBUTION IS OVER THE CLASSES!\n",
    "4. We train the student network using this high temperature value \n",
    "5. We use a temperature value of 1 again when evaluating the student model\n",
    "6. We train a control network (same architecture as the student) using ordinary training techniques to get a comparison\n",
    "\n",
    "### Why do we use Knowledge Distillation?:\n",
    "-We use knowledge distillation in order to get better results using smaller networks that take far less time and computing power to run! These small and high performing networks are ideal for making marketable products that work well in real time!\n",
    "\n",
    "### Papers and Sources Cited that Helped with Ideas and Implementations:\n",
    "-https://cs230.stanford.edu/files_winter_2018/projects/6940224.pdf\n",
    "\n",
    "-https://machinelearningmastery.com/cross-entropy-for-machine-learning/\n",
    "\n",
    "-https://keras.io/examples/vision/knowledge_distillation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320a86f7",
   "metadata": {},
   "source": [
    "## Part 1: General Distillation Implementation and Baseline Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff812297",
   "metadata": {},
   "source": [
    "First we load the MNIST training set and normalize the pixel values by dividing by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39a50389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below We load in the mnist dataset that we have used in previous weeks so we don't \n",
    "#do much preprocessing or EDA this week!\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalizing our data\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
    "\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "x_test = np.reshape(x_test, (-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1769f1e",
   "metadata": {},
   "source": [
    "Next we create the model architectures for the Large Neural Network (teacher model), the Small Neural Network (student model), and a copy of the Small Neural Network (control model).\n",
    "Then at the end we create copies of the student model for later when we are optimizing the temperature and and alpha in part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03117d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-06 09:41:25.913131: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'student0': <tensorflow.python.keras.engine.sequential.Sequential object at 0x7ffc49c78ee0>, 'student1': <tensorflow.python.keras.engine.sequential.Sequential object at 0x7ffc56395520>, 'student2': <tensorflow.python.keras.engine.sequential.Sequential object at 0x7ffc563b4340>, 'student3': <tensorflow.python.keras.engine.sequential.Sequential object at 0x7ffc563b83d0>, 'student4': <tensorflow.python.keras.engine.sequential.Sequential object at 0x7ffc563d1580>, 'student5': <tensorflow.python.keras.engine.sequential.Sequential object at 0x7ffc563e83a0>, 'student6': <tensorflow.python.keras.engine.sequential.Sequential object at 0x7ffc563fbaf0>, 'student7': <tensorflow.python.keras.engine.sequential.Sequential object at 0x7ffc56454bb0>, 'student8': <tensorflow.python.keras.engine.sequential.Sequential object at 0x7ffc564630d0>, 'student9': <tensorflow.python.keras.engine.sequential.Sequential object at 0x7ffc56485df0>}\n"
     ]
    }
   ],
   "source": [
    "# Create the teacher network model\n",
    "teacher = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),            #Note the large size of this layer\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),    #And of this layer!\n",
    "        layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10),\n",
    "    ],\n",
    "    name=\"teacher\",\n",
    ")\n",
    "\n",
    "# Create the student\n",
    "student = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),      #Note that this student network has smaller  \n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),   \n",
    "        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),      # layers in these parts!!\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10),\n",
    "    ],\n",
    "    name=\"student\",\n",
    ")\n",
    "\n",
    "#Creating a copy of the student model to be the control model!\n",
    "controlModel = keras.models.clone_model(student)\n",
    "\n",
    "#Here we are creating copies of the student model for later when we are optimizing temperature and and alpha\n",
    "studentDict = {}\n",
    "numStudents = 10\n",
    "for i in range(numStudents):\n",
    "    currString = 'student'+str(i)\n",
    "    studentDict[currString] = keras.models.clone_model(student)\n",
    "\n",
    "print(studentDict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50964eb2",
   "metadata": {},
   "source": [
    "Next we train the large teaching model in the usual way since we assume that this model is fixed.  It is important to note that this model takes a long time to train nearly 2.5 minutes per epoch!!(this is what we are trying to eliminate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5ed8536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-06 09:41:26.836196: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 0.1428 - sparse_categorical_accuracy: 0.9568\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 161s 86ms/step - loss: 0.0918 - sparse_categorical_accuracy: 0.9720\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 171s 91ms/step - loss: 0.0812 - sparse_categorical_accuracy: 0.9763\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 171s 91ms/step - loss: 0.0756 - sparse_categorical_accuracy: 0.9790\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 164s 88ms/step - loss: 0.0681 - sparse_categorical_accuracy: 0.9809\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.0834 - sparse_categorical_accuracy: 0.9777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08337046951055527, 0.9776999950408936]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train teacher model in the basic way \n",
    "teacher.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train and evaluate teacher on data.\n",
    "teacher.fit(x_train, y_train, epochs=5)\n",
    "teacher.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d7bfd",
   "metadata": {},
   "source": [
    "Below is our class implementation of the knowledge distillation algorithm that was presented in the slides.  We will input our student and our trained teacher into this class in order to make our distilled student model (where the student model is using the smoothed probability classification output of the teacher model in order to learn from it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e9ce886",
   "metadata": {},
   "outputs": [],
   "source": [
    "class distillationModel(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(distillationModel, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=10,\n",
    "    ):\n",
    "        super(distillationModel, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    #This method does a forward pass of the \"student\" and \"teacher\".  Only student weights are updated though\n",
    "    #thus we only calculate gradients for the \"student\"\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        teacher_predictions = self.teacher(x, training=False)  #\"teachers forward pass\"\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_predictions = self.student(x, training=True)  #\"students forward pass\"\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)  #student and distillation losses\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "            )\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "        trainable_vars = self.student.trainable_variables     #student gradient\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars)) #updating the weights with the gradient applied\n",
    "        \n",
    "        self.compiled_metrics.update_state(y, student_predictions)  #updating the metrics\n",
    "\n",
    "        results = {m.name: m.result() for m in self.metrics}       #returning the performance currently in a dictionary\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    #In this function the student model is evaluated on the current dataset\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_prediction = self.student(x, training=False) \n",
    "        student_loss = self.student_loss_fn(y, y_prediction)   \n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1ab3ba",
   "metadata": {},
   "source": [
    "-Next we create the definition of our distillation (student model) using the class definition above and we set our hyperparameters to be temperature=10 and alpha=0.1\n",
    "\n",
    "-Looking at our results we can see that this distillation model achieved 97.53% sparse categorical accuracy on the MNIST dataset.  This is great because this network was so much smaller that it took only a fraction of the time to train and run (looking below we can see it was roughly 46 seconds per epoch as opposed to the 2.5 minutes for the large teacher network)\n",
    "\n",
    "-Also it achieved this level of accuracy after only 3 epochs instead of 5!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e2008ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - sparse_categorical_accuracy: 0.9136 - student_loss: 0.3650 - distillation_loss: 0.1035\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 48s 25ms/step - sparse_categorical_accuracy: 0.9705 - student_loss: 0.1126 - distillation_loss: 0.0273\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - sparse_categorical_accuracy: 0.9760 - student_loss: 0.0880 - distillation_loss: 0.0201\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9753 - student_loss: 0.0917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9753000140190125, 5.8156940212938935e-05]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and compile distiller\n",
    "distiller = distillationModel(student=student, teacher=teacher)\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")\n",
    "\n",
    "# Distill teacher to student\n",
    "distiller.fit(x_train, y_train, epochs=3)\n",
    "\n",
    "# Evaluate student on test dataset\n",
    "distiller.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fe6f99",
   "metadata": {},
   "source": [
    "Finally, taking a look at the control models results below we were initially very surprised by the results! This is because the control small model is actually able to achieve 97.8% accuracy after only 3 epochs each taking 8 seconds to run which is better than the distillation model.\n",
    "\n",
    "-However thinking about it more we remembered the temperature and alpha hyperparameters that the distillation uses and realized that we may have not picked the optimal values (10 and 0.1) for these.  \n",
    "\n",
    "-This leads us into part 2 where we try to tune the temperature an alpha hyperparameters to improve the distillation model to the point of outperforming the control model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9417b91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2418 - sparse_categorical_accuracy: 0.9277\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0923 - sparse_categorical_accuracy: 0.9720\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0739 - sparse_categorical_accuracy: 0.9768\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0655 - sparse_categorical_accuracy: 0.9782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06546541303396225, 0.9782000184059143]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Control Model Using the Student Clone from Earlier\n",
    "controlModel.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train and evaluate student trained from scratch.\n",
    "controlModel.fit(x_train, y_train, epochs=3)\n",
    "controlModel.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1a413f",
   "metadata": {},
   "source": [
    "## Part 2: Optimizing the Values of Temperature and Alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d04955",
   "metadata": {},
   "source": [
    "In this section we will now attempt to find the optimal values for the temperature and alpha parameter in the previous problem that maximize the accuracy performance of our small (student) neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df4e9f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9221 - student_loss: 0.2820 - distillation_loss: 0.2228\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 48s 25ms/step - sparse_categorical_accuracy: 0.9696 - student_loss: 0.1108 - distillation_loss: 0.0654\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 46s 25ms/step - sparse_categorical_accuracy: 0.9747 - student_loss: 0.0901 - distillation_loss: 0.0460\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9759 - student_loss: 0.0859\n",
      "Current Distillation evaluation is: [0.9758999943733215, 0.00019955712195951492]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - sparse_categorical_accuracy: 0.9773 - student_loss: 0.0855 - distillation_loss: 0.0331\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9794 - student_loss: 0.0759 - distillation_loss: 0.0262\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 46s 25ms/step - sparse_categorical_accuracy: 0.9811 - student_loss: 0.0690 - distillation_loss: 0.0226\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9773 - student_loss: 0.0857\n",
      "Current Distillation evaluation is: [0.9772999882698059, 0.00024376359942834824]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9820 - student_loss: 0.0651 - distillation_loss: 0.0184\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 45s 24ms/step - sparse_categorical_accuracy: 0.9823 - student_loss: 0.0612 - distillation_loss: 0.0167\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 46s 25ms/step - sparse_categorical_accuracy: 0.9833 - student_loss: 0.0575 - distillation_loss: 0.0156\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9787 - student_loss: 0.0814\n",
      "Current Distillation evaluation is: [0.9786999821662903, 0.00020254083210602403]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9847 - student_loss: 0.0529 - distillation_loss: 0.0125\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9852 - student_loss: 0.0500 - distillation_loss: 0.0119\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 45s 24ms/step - sparse_categorical_accuracy: 0.9865 - student_loss: 0.0475 - distillation_loss: 0.0116\n",
      "313/313 [==============================] - 1s 1ms/step - sparse_categorical_accuracy: 0.9795 - student_loss: 0.0789\n",
      "Current Distillation evaluation is: [0.9794999957084656, 0.00015314886695705354]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - sparse_categorical_accuracy: 0.9865 - student_loss: 0.0447 - distillation_loss: 0.0092\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - sparse_categorical_accuracy: 0.9872 - student_loss: 0.0420 - distillation_loss: 0.0089\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9880 - student_loss: 0.0403 - distillation_loss: 0.0087\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9811 - student_loss: 0.0771\n",
      "Current Distillation evaluation is: [0.9811000227928162, 5.215712735662237e-05]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9883 - student_loss: 0.0386 - distillation_loss: 0.0070\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 44s 23ms/step - sparse_categorical_accuracy: 0.9891 - student_loss: 0.0354 - distillation_loss: 0.0070\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9893 - student_loss: 0.0354 - distillation_loss: 0.0069\n",
      "313/313 [==============================] - 1s 1ms/step - sparse_categorical_accuracy: 0.9796 - student_loss: 0.0758\n",
      "Current Distillation evaluation is: [0.9796000123023987, 9.621180652175099e-05]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - sparse_categorical_accuracy: 0.9898 - student_loss: 0.0335 - distillation_loss: 0.0056\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9905 - student_loss: 0.0316 - distillation_loss: 0.0056\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9911 - student_loss: 0.0297 - distillation_loss: 0.0056\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9819 - student_loss: 0.0723\n",
      "Current Distillation evaluation is: [0.9818999767303467, 5.076875095255673e-05]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - sparse_categorical_accuracy: 0.9908 - student_loss: 0.0292 - distillation_loss: 0.0047\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - sparse_categorical_accuracy: 0.9917 - student_loss: 0.0275 - distillation_loss: 0.0047\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9920 - student_loss: 0.0261 - distillation_loss: 0.0047\n",
      "313/313 [==============================] - 1s 1ms/step - sparse_categorical_accuracy: 0.9826 - student_loss: 0.0709\n",
      "Current Distillation evaluation is: [0.9825999736785889, 8.956550300354138e-05]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 49s 26ms/step - sparse_categorical_accuracy: 0.9921 - student_loss: 0.0256 - distillation_loss: 0.0040\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 48s 25ms/step - sparse_categorical_accuracy: 0.9929 - student_loss: 0.0243 - distillation_loss: 0.0039\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9926 - student_loss: 0.0232 - distillation_loss: 0.0039\n",
      "313/313 [==============================] - 1s 1ms/step - sparse_categorical_accuracy: 0.9828 - student_loss: 0.0662\n",
      "Current Distillation evaluation is: [0.9828000068664551, 0.00019859040912706405]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9926 - student_loss: 0.0232 - distillation_loss: 0.0034\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - sparse_categorical_accuracy: 0.9937 - student_loss: 0.0205 - distillation_loss: 0.0034\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 45s 24ms/step - sparse_categorical_accuracy: 0.9935 - student_loss: 0.0204 - distillation_loss: 0.0034\n",
      "313/313 [==============================] - 1s 1ms/step - sparse_categorical_accuracy: 0.9829 - student_loss: 0.0678\n",
      "Current Distillation evaluation is: [0.9829000234603882, 0.0002673761919140816]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - sparse_categorical_accuracy: 0.9213 - student_loss: 0.2828 - distillation_loss: 0.2254\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 45s 24ms/step - sparse_categorical_accuracy: 0.9711 - student_loss: 0.1028 - distillation_loss: 0.0565\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 45s 24ms/step - sparse_categorical_accuracy: 0.9760 - student_loss: 0.0867 - distillation_loss: 0.0422\n",
      "313/313 [==============================] - 1s 1ms/step - sparse_categorical_accuracy: 0.9794 - student_loss: 0.0788\n",
      "Current Distillation evaluation is: [0.9793999791145325, 0.00013211776968091726]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 45s 24ms/step - sparse_categorical_accuracy: 0.9787 - student_loss: 0.0788 - distillation_loss: 0.0301\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 44s 24ms/step - sparse_categorical_accuracy: 0.9807 - student_loss: 0.0700 - distillation_loss: 0.0235\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 44s 24ms/step - sparse_categorical_accuracy: 0.9823 - student_loss: 0.0636 - distillation_loss: 0.0202\n",
      "313/313 [==============================] - 1s 1ms/step - sparse_categorical_accuracy: 0.9805 - student_loss: 0.0780\n",
      "Current Distillation evaluation is: [0.9804999828338623, 5.1049482863163576e-05]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 44s 23ms/step - sparse_categorical_accuracy: 0.9826 - student_loss: 0.0590 - distillation_loss: 0.0161\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 44s 23ms/step - sparse_categorical_accuracy: 0.9843 - student_loss: 0.0549 - distillation_loss: 0.0143\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 43s 23ms/step - sparse_categorical_accuracy: 0.9846 - student_loss: 0.0523 - distillation_loss: 0.0132\n",
      "313/313 [==============================] - 1s 1ms/step - sparse_categorical_accuracy: 0.9801 - student_loss: 0.0736\n",
      "Current Distillation evaluation is: [0.9800999760627747, 0.00012476825213525444]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 45s 24ms/step - sparse_categorical_accuracy: 0.9858 - student_loss: 0.0486 - distillation_loss: 0.0104\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 44s 23ms/step - sparse_categorical_accuracy: 0.9862 - student_loss: 0.0463 - distillation_loss: 0.0098 0s - sparse_categorical_accuracy: 0.9861 - student_loss: 0.0467 - distillation_los\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 44s 23ms/step - sparse_categorical_accuracy: 0.9871 - student_loss: 0.0445 - distillation_loss: 0.0094\n",
      "313/313 [==============================] - 1s 1ms/step - sparse_categorical_accuracy: 0.9814 - student_loss: 0.0743\n",
      "Current Distillation evaluation is: [0.9814000129699707, 0.00016035143926274031]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 44s 23ms/step - sparse_categorical_accuracy: 0.9876 - student_loss: 0.0412 - distillation_loss: 0.0075 3s - sparse_categorical_acc\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 44s 23ms/step - sparse_categorical_accuracy: 0.9884 - student_loss: 0.0389 - distillation_loss: 0.0073\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 46s 25ms/step - sparse_categorical_accuracy: 0.9890 - student_loss: 0.0371 - distillation_loss: 0.0072\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9828 - student_loss: 0.0694\n",
      "Current Distillation evaluation is: [0.9828000068664551, 0.00014201371232047677]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 48s 25ms/step - sparse_categorical_accuracy: 0.9891 - student_loss: 0.0356 - distillation_loss: 0.0058\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 46s 25ms/step - sparse_categorical_accuracy: 0.9896 - student_loss: 0.0332 - distillation_loss: 0.0058\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 46s 25ms/step - sparse_categorical_accuracy: 0.9899 - student_loss: 0.0317 - distillation_loss: 0.0058\n",
      "313/313 [==============================] - 1s 1ms/step - sparse_categorical_accuracy: 0.9837 - student_loss: 0.0663\n",
      "Current Distillation evaluation is: [0.9836999773979187, 3.942491821362637e-05]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - sparse_categorical_accuracy: 0.9904 - student_loss: 0.0309 - distillation_loss: 0.0048\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 44s 23ms/step - sparse_categorical_accuracy: 0.9910 - student_loss: 0.0289 - distillation_loss: 0.0048\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - sparse_categorical_accuracy: 0.9916 - student_loss: 0.0275 - distillation_loss: 0.0047\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9831 - student_loss: 0.0715\n",
      "Current Distillation evaluation is: [0.9830999970436096, 3.142429341096431e-05]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - sparse_categorical_accuracy: 0.9918 - student_loss: 0.0257 - distillation_loss: 0.0040\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 46s 25ms/step - sparse_categorical_accuracy: 0.9919 - student_loss: 0.0251 - distillation_loss: 0.0040\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9925 - student_loss: 0.0240 - distillation_loss: 0.0040\n",
      "313/313 [==============================] - 1s 1ms/step - sparse_categorical_accuracy: 0.9832 - student_loss: 0.0665\n",
      "Current Distillation evaluation is: [0.9832000136375427, 5.8401026763021946e-05]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - sparse_categorical_accuracy: 0.9926 - student_loss: 0.0230 - distillation_loss: 0.0034\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 45s 24ms/step - sparse_categorical_accuracy: 0.9932 - student_loss: 0.0216 - distillation_loss: 0.0034\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 45s 24ms/step - sparse_categorical_accuracy: 0.9938 - student_loss: 0.0207 - distillation_loss: 0.0034\n",
      "313/313 [==============================] - 1s 1ms/step - sparse_categorical_accuracy: 0.9814 - student_loss: 0.0725\n",
      "Current Distillation evaluation is: [0.9814000129699707, 7.111158629413694e-05]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - sparse_categorical_accuracy: 0.9933 - student_loss: 0.0210 - distillation_loss: 0.0030\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 45s 24ms/step - sparse_categorical_accuracy: 0.9944 - student_loss: 0.0187 - distillation_loss: 0.0029\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 45s 24ms/step - sparse_categorical_accuracy: 0.9945 - student_loss: 0.0183 - distillation_loss: 0.0030\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9822 - student_loss: 0.0698\n",
      "Current Distillation evaluation is: [0.982200026512146, 3.158852632623166e-05]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 48s 26ms/step - sparse_categorical_accuracy: 0.9110 - student_loss: 0.3304 - distillation_loss: 0.2584\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9681 - student_loss: 0.1161 - distillation_loss: 0.0695\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9751 - student_loss: 0.0888 - distillation_loss: 0.0453\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9741 - student_loss: 0.0898\n",
      "Current Distillation evaluation is: [0.9740999937057495, 9.425570169696584e-05]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9773 - student_loss: 0.0846 - distillation_loss: 0.0330\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9797 - student_loss: 0.0748 - distillation_loss: 0.0260\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 48s 26ms/step - sparse_categorical_accuracy: 0.9804 - student_loss: 0.0685 - distillation_loss: 0.0220\n",
      "313/313 [==============================] - 1s 1ms/step - sparse_categorical_accuracy: 0.9803 - student_loss: 0.0751\n",
      "Current Distillation evaluation is: [0.9803000092506409, 0.0002843035908881575]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 50s 27ms/step - sparse_categorical_accuracy: 0.9817 - student_loss: 0.0635 - distillation_loss: 0.0174\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 48s 25ms/step - sparse_categorical_accuracy: 0.9831 - student_loss: 0.0585 - distillation_loss: 0.0154\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 49s 26ms/step - sparse_categorical_accuracy: 0.9838 - student_loss: 0.0550 - distillation_loss: 0.0142\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9796 - student_loss: 0.0743\n",
      "Current Distillation evaluation is: [0.9796000123023987, 0.00022466908558271825]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 48s 25ms/step - sparse_categorical_accuracy: 0.9845 - student_loss: 0.0514 - distillation_loss: 0.0112\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - sparse_categorical_accuracy: 0.9855 - student_loss: 0.0476 - distillation_loss: 0.0106\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 52s 28ms/step - sparse_categorical_accuracy: 0.9866 - student_loss: 0.0457 - distillation_loss: 0.0101\n",
      "313/313 [==============================] - 2s 5ms/step - sparse_categorical_accuracy: 0.9822 - student_loss: 0.0700A: 0s - sparse_categorical_accuracy: 0.9810 - student_loss: 0.\n",
      "Current Distillation evaluation is: [0.982200026512146, 0.0002035626384895295]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 48s 25ms/step - sparse_categorical_accuracy: 0.9874 - student_loss: 0.0431 - distillation_loss: 0.0081\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 61s 32ms/step - sparse_categorical_accuracy: 0.9879 - student_loss: 0.0405 - distillation_loss: 0.0079\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 61s 32ms/step - sparse_categorical_accuracy: 0.9882 - student_loss: 0.0388 - distillation_loss: 0.0077\n",
      "313/313 [==============================] - 1s 1ms/step - sparse_categorical_accuracy: 0.9831 - student_loss: 0.0654\n",
      "Current Distillation evaluation is: [0.9830999970436096, 0.0002271479315822944]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 46s 25ms/step - sparse_categorical_accuracy: 0.9887 - student_loss: 0.0371 - distillation_loss: 0.0062\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - sparse_categorical_accuracy: 0.9894 - student_loss: 0.0345 - distillation_loss: 0.0061\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 48s 26ms/step - sparse_categorical_accuracy: 0.9899 - student_loss: 0.0338 - distillation_loss: 0.0061\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9835 - student_loss: 0.0623\n",
      "Current Distillation evaluation is: [0.9835000038146973, 0.00019845484348479658]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 50s 27ms/step - sparse_categorical_accuracy: 0.9905 - student_loss: 0.0315 - distillation_loss: 0.0050\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 48s 26ms/step - sparse_categorical_accuracy: 0.9907 - student_loss: 0.0299 - distillation_loss: 0.0050\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 48s 26ms/step - sparse_categorical_accuracy: 0.9914 - student_loss: 0.0287 - distillation_loss: 0.0050\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9818 - student_loss: 0.0682\n",
      "Current Distillation evaluation is: [0.9818000197410583, 0.00023517681984230876]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 48s 25ms/step - sparse_categorical_accuracy: 0.9915 - student_loss: 0.0275 - distillation_loss: 0.0042\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 45s 24ms/step - sparse_categorical_accuracy: 0.9923 - student_loss: 0.0255 - distillation_loss: 0.0042\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - sparse_categorical_accuracy: 0.9922 - student_loss: 0.0251 - distillation_loss: 0.0042\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9821 - student_loss: 0.0698\n",
      "Current Distillation evaluation is: [0.9821000099182129, 8.10353085398674e-05]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 44s 23ms/step - sparse_categorical_accuracy: 0.9927 - student_loss: 0.0242 - distillation_loss: 0.0036\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 44s 23ms/step - sparse_categorical_accuracy: 0.9930 - student_loss: 0.0226 - distillation_loss: 0.0035\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 46s 25ms/step - sparse_categorical_accuracy: 0.9937 - student_loss: 0.0216 - distillation_loss: 0.0036\n",
      "313/313 [==============================] - 1s 1ms/step - sparse_categorical_accuracy: 0.9834 - student_loss: 0.0671\n",
      "Current Distillation evaluation is: [0.9833999872207642, 5.345668978407048e-05]\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 45s 24ms/step - sparse_categorical_accuracy: 0.9934 - student_loss: 0.0218 - distillation_loss: 0.0031\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 45s 24ms/step - sparse_categorical_accuracy: 0.9940 - student_loss: 0.0192 - distillation_loss: 0.0031\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - sparse_categorical_accuracy: 0.9942 - student_loss: 0.0189 - distillation_loss: 0.0031\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9838 - student_loss: 0.0638\n",
      "Current Distillation evaluation is: [0.9837999939918518, 5.0520338845672086e-05]\n"
     ]
    }
   ],
   "source": [
    "alphaArr = [0.05,0.5,0.95]\n",
    "numAlphas = len(alphaArr)\n",
    "alphaTempArr = np.zeros((numAlphas,numStudents))\n",
    "tempArr = []\n",
    "\n",
    "accuracyArr = []\n",
    "for i in range(numAlphas):\n",
    "    for j in range(numStudents):\n",
    "        currString = 'student'+str(i)\n",
    "        currDistiller = distillationModel(student=studentDict[currString], teacher=teacher)\n",
    "        currDistiller.compile(\n",
    "            optimizer=keras.optimizers.Adam(),\n",
    "            metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "            student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "            alpha=0.1,\n",
    "            temperature=((j+1)*2),\n",
    "        )\n",
    "            # Distill teacher to student\n",
    "        currDistiller.fit(x_train, y_train, epochs=3)\n",
    "\n",
    "        # Evaluate student on test dataset\n",
    "        currEval = currDistiller.evaluate(x_test, y_test)\n",
    "        print(f\"Current Distillation evaluation is: {currEval}\")\n",
    "\n",
    "        #Appending current temperature to temperature array\n",
    "        tempArr.append((j+1)*2)\n",
    "        \n",
    "        alphaTempArr[i,j] = currEval[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8896330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plzzz\n",
      "[[0.97589999 0.97729999 0.97869998 0.9795     0.98110002 0.97960001\n",
      "  0.98189998 0.98259997 0.98280001 0.98290002]\n",
      " [0.97939998 0.98049998 0.98009998 0.98140001 0.98280001 0.98369998\n",
      "  0.9831     0.98320001 0.98140001 0.98220003]\n",
      " [0.97409999 0.98030001 0.97960001 0.98220003 0.9831     0.9835\n",
      "  0.98180002 0.98210001 0.98339999 0.98379999]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Below is the results of each of the distillation 'students' after 3 epochs of training in an array\")\n",
    "print(alphaTempArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be612eb",
   "metadata": {},
   "source": [
    "In the cell below we visualize the results using a heatmap produced by seaborn.  We can see how the different distillation \"student\" models do as we vary the input parameters for the temperature and alpha.\n",
    "\n",
    "The largest thing to note is the really light square in the middle where the temperature parameter was 12 and the alpha was 0.5.  This small network distillation model had a testing accuracy of over 98.2% after just 3 epochs of learning from the teacher model!  This is much better than the small network model training on its own and even the results from the teacher model.  \n",
    "\n",
    "From what we have learned we believe that these distillation models are able to perform so well with so little training because they learn things parts of the probability distribution of the teacher model where it is unsure (where it might be predicting a 5% probability that an image is a 5 and a 3% probability that the image is a 2) as well as the correct parts of the teachers probability distribution (for example when it is predicting 90% that an image is classified as a 7 or something)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95306254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.heatmap(alphaTempArr, linewidth=0.5)\n",
    "ax.set_title('Final Accuracy of Distillation \"Student Models\" Varying Temperature and Alpha Parameters')\n",
    "ax.set(xlabel='Temperature', ylabel='Alpha')\n",
    "ax.set_xticks(range(10))\n",
    "#ax.set_xticklabels('2','4','6','8','10','12','14','16','18','20')\n",
    "temps = ['2','4','6','8','10','12','14','16','18','20']\n",
    "ax.set_xticks(np.arange(len(temps)))\n",
    "ax.set_xticklabels(temps)\n",
    "alphas = ['0.05','0.5','0.95']\n",
    "ax.set_yticks(np.arange(len(alphas)))\n",
    "ax.set_yticklabels(alphas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d62065c",
   "metadata": {},
   "source": [
    "## Part 3: Testing Generalizability to Cifar-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75fb7f0",
   "metadata": {},
   "source": [
    "In this section we test how generalizable the distillation model we just found is!  In order to do this we use the Cifar10 dataset instead of the MNIST dataset.  Then we create a distillation model using the hyperparameters of 12 for temperature and 0.5 for alpha that we found worked so well on the MNIST dataset.\n",
    "\n",
    "Then our objective is to compare the results of this distillation model to the results of training a normal small network to see if these parameters that we tuned on a different dataset generalize well, or whether it is necessary to go through the hyperparameter tuning process again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c43335da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the cifar10 dataset now\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize data\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = np.reshape(x_train, (-1, 32, 32, 3))\n",
    "\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "x_test = np.reshape(x_test, (-1, 32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e9c0c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    }
   ],
   "source": [
    "# Create the teacher\n",
    "teacher = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(32, 32, 3)),\n",
    "        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
    "        layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10),\n",
    "    ],\n",
    "    name=\"teacher\",\n",
    ")\n",
    "\n",
    "# Create the student\n",
    "student = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(32, 32, 3)),\n",
    "        layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
    "        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10),\n",
    "    ],\n",
    "    name=\"student\",\n",
    ")\n",
    "\n",
    "# Clone the student for the control model\n",
    "controlModel = keras.models.clone_model(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff71ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class distillationModel(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(distillationModel, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.5,\n",
    "        temperature=12,\n",
    "    ):\n",
    "        super(distillationModel, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    #This method does a forward pass of the \"student\" and \"teacher\".  Only student weights are updated though\n",
    "    #thus we only calculate gradients for the \"student\"\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        teacher_predictions = self.teacher(x, training=False)  #\"teachers forward pass\"\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_predictions = self.student(x, training=True)  #\"students forward pass\"\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)  #student and distillation losses\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "            )\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "        trainable_vars = self.student.trainable_variables     #student gradient\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars)) #updating the weights with the gradient applied\n",
    "        \n",
    "        self.compiled_metrics.update_state(y, student_predictions)  #updating the metrics\n",
    "\n",
    "        results = {m.name: m.result() for m in self.metrics}       #returning the performance currently in a dictionary\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    #In this function the student model is evaluated on the current dataset\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_prediction = self.student(x, training=False) \n",
    "        student_loss = self.student_loss_fn(y, y_prediction)   \n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a40ffa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 178s 114ms/step - loss: 1.4563 - sparse_categorical_accuracy: 0.4918\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 180s 115ms/step - loss: 1.2444 - sparse_categorical_accuracy: 0.5755\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 186s 119ms/step - loss: 1.1691 - sparse_categorical_accuracy: 0.6023\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 199s 127ms/step - loss: 1.1231 - sparse_categorical_accuracy: 0.6176\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 187s 119ms/step - loss: 1.0817 - sparse_categorical_accuracy: 0.6304\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 1.2776 - sparse_categorical_accuracy: 0.5801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2775644063949585, 0.5800999999046326]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train teacher as usual\n",
    "teacher.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train and evaluate teacher on data.\n",
    "teacher.fit(x_train, y_train, epochs=5)\n",
    "teacher.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8657fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 54s 34ms/step - sparse_categorical_accuracy: 0.6144 - student_loss: 1.1126 - distillation_loss: 0.0086\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 66s 42ms/step - sparse_categorical_accuracy: 0.6242 - student_loss: 1.0892 - distillation_loss: 0.0085\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 49s 32ms/step - sparse_categorical_accuracy: 0.6299 - student_loss: 1.0730 - distillation_loss: 0.0084\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 50s 32ms/step - sparse_categorical_accuracy: 0.6334 - student_loss: 1.0587 - distillation_loss: 0.0083\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 51s 33ms/step - sparse_categorical_accuracy: 0.6413 - student_loss: 1.0436 - distillation_loss: 0.0082\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.6135 - student_loss: 1.1276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6134999990463257, 1.1059038639068604]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and compile distiller\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.5,\n",
    "    temperature=12,\n",
    ")\n",
    "\n",
    "# Distill teacher to student\n",
    "distiller.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# Evaluate student on test dataset\n",
    "distiller.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f20a96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1021 - sparse_categorical_accuracy: 0.6196\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.0818 - sparse_categorical_accuracy: 0.6257\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.0636 - sparse_categorical_accuracy: 0.6320\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0498 - sparse_categorical_accuracy: 0.6361\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.0327 - sparse_categorical_accuracy: 0.6433\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.1426 - sparse_categorical_accuracy: 0.6052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1425632238388062, 0.6051999926567078]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train student as done usually\n",
    "controlModel.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train and evaluate student trained from scratch.\n",
    "controlModel.fit(x_train, y_train, epochs=5)\n",
    "#controlModel.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b816c",
   "metadata": {},
   "source": [
    "## Conclusions from part 3 looking at results:\n",
    "-From above we can see that the \"teacher\" neural network had a categorical accuracy of 58.0% after training and the epochs took approximately 187 seconds to train.  The Distillation \"student\" model had a categorical accuracy of 61.3% accuracy after training and the epochs took around 50 seconds to train.  Finally the control model had 60.5% accuracy and took around 9 seconds to train.  From this we can conclude that the distillation parameters we found in part 2 seem to generalize reasonably well since they came up with a model of higher accuracy than the control and the \"teacher\" network.  Also we can see the advantage of these distillation networks with giving high accuracy results with less training time than the massive networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6e843b",
   "metadata": {},
   "source": [
    "## Part 4: Using an Ensemble Network as the Teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c50f313",
   "metadata": {},
   "source": [
    "Due to long training times and some debugging errors we were unable to finish this section this week.  However here is an explanation of what we are planning to do in this section and would really appreciate some feedback on if you think it is an interesting idea to pursue or if we should move on and focus our efforts elsewhere!\n",
    "\n",
    "-Our plan for this section was to replace the large \"teacher\" network that we had been using in the previous sections with an ensemble of \"state of the art\" neural networks such as VGG19, Resnet50, and Nasnet that all had their predefined weights from training on the imagenet dataset.  We would then use a similar student model and see how it fairs when all of these extremely complex models are ensembled together into one really good teacher model.  We figured it might even suffice just to use one of these \"state of the art\" neural networks with predefined weights as the teacher and see how well it can teach our small neural network on the cifar10 dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
